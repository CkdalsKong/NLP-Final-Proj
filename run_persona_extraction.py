import json
import faiss
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
from openai import OpenAI

client = OpenAI(api_key="YOUR_API_KEY")
model = SentenceTransformer("all-MiniLM-L6-v2")

def llm_persona_prompt(name, utterances):
    joined = "\n".join(utterances)
    prompt = f"""
ë‹¤ìŒì€ ì–´ë–¤ ì‚¬ëŒì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ì‚¬ëŒì˜ ë‚˜ì´ëŒ€, ì„±ë³„, ì„±ê²©, ì·¨í–¥ ë“±ì„ ì¶”ë¡ í•˜ì„¸ìš”.

[ëŒ€í™”]
{joined}

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
ë‚˜ì´ëŒ€: 20ëŒ€, ì„±ë³„: ì—¬ì„±, ì„±ê²©: ì™¸í–¥ì , ì·¨í–¥: íŠ¸ë Œë””í•œ ê²ƒ
"""
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content.strip()


def run_a1(dialogue):
    utterances = [f'{t["speaker"]}: {t["text"]}' for t in dialogue]
    return llm_persona_prompt("Full Dialogue", utterances)


def run_a2(dialogue, speaker):
    speaker_utterances = [t["text"] for t in dialogue if t["speaker"] == speaker]
    return llm_persona_prompt(speaker, speaker_utterances)


def run_a3(dialogue, speaker, chunk_size=3):
    chunks = []
    cur_chunk = []
    for turn in dialogue:
        if turn["speaker"] == speaker:
            cur_chunk.append(turn["text"])
        if len(cur_chunk) == chunk_size:
            chunks.append(cur_chunk)
            cur_chunk = []
    if cur_chunk:
        chunks.append(cur_chunk)

    results = []
    for chunk in chunks:
        result = llm_persona_prompt(speaker, chunk)
        results.append(result)
    return "\n\n".join(results)


def run_a4_rag(dialogue_id, speaker, style_summary_path, embedding_path, index_path, metadata_path):
    # 1. style summary ë¡œë“œ
    style_data = {}
    with open(style_summary_path, "r", encoding="utf-8") as f:
        for line in f:
            d = json.loads(line)
            if d["dialogue_id"] == dialogue_id and d["speaker"] == speaker:
                style_data = d
                break

    if not style_data:
        return f"âŒ style summary not found for {speaker}"

    query = style_data["style_summary"]
    embedding = model.encode([query])

    # 2. FAISS ê²€ìƒ‰
    index = faiss.read_index(index_path)
    D, I = index.search(embedding, k=3)

    # 3. metadata ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜
    with open(metadata_path, "r", encoding="utf-8") as f:
        metadata = [json.loads(l) for l in f]
    retrieved = [metadata[i] for i in I[0]]

    # 4. prompt êµ¬ì„±
    retrieved_str = "\n".join([
        f"- ë‚˜ì´ëŒ€: {r.get('age', '?')}, ì„±ê²©: ?, ìŠ¬ë­: {', '.join(r['slang_used'])}, ìš”ì•½: {r['style_summary']}"
        for r in retrieved
    ])

    prompt = f"""
ë‹¤ìŒì€ í™”ì {speaker}ì˜ ëŒ€í™” ìŠ¤íƒ€ì¼ê³¼ ìœ ì‚¬í•œ í˜ë¥´ì†Œë‚˜ ì˜ˆì‹œì…ë‹ˆë‹¤.
ì´ë“¤ì„ ì°¸ê³ í•˜ì—¬ {speaker}ì˜ í˜ë¥´ì†Œë‚˜ë¥¼ ì¶”ë¡ í•˜ì„¸ìš”.

[Retrieved Examples]
{retrieved_str}

[Output format]
ë‚˜ì´ëŒ€: ..., ì„±ë³„: ..., ì„±ê²©: ..., ì·¨í–¥: ...
    """
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content.strip()


def run_all_methods(dialogue_file: str, base_dir: str):
    dialogue_id = Path(dialogue_file).parent.name
    with open(dialogue_file, "r", encoding="utf-8") as f:
        dialogue = json.load(f)

    speakers = list(set(t["speaker"] for t in dialogue))
    results = {}

    for speaker in speakers:
        print(f"\nğŸ§‘â€ğŸ’¬ ì¶”ë¡  ì¤‘: {speaker}")
        results[speaker] = {
            "A1_long_context": run_a1(dialogue),
            "A2_speaker_only": run_a2(dialogue, speaker),
            "A3_chunk_based": run_a3(dialogue, speaker),
            "A4_rag_based": run_a4_rag(
                dialogue_id=dialogue_id,
                speaker=speaker,
                style_summary_path=f"{base_dir}/style_summary.jsonl",
                embedding_path=f"{base_dir}/style_embeddings.npy",
                index_path=f"{base_dir}/style_faiss.index",
                metadata_path=f"{base_dir}/style_metadata.jsonl"
            )
        }

    # ì €ì¥
    out_path = Path(base_dir) / "persona_extraction_result.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ëª¨ë“  ë°©ì‹ ì¶”ë¡  ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {out_path}")
